\documentclass[letterpaper,12pt]{article}

\usepackage{ucs}
\usepackage[utf8x]{inputenc}
\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage[margin=1in]{geometry}
\usepackage{enumerate}

\newcommand{\abs}[1]{\lvert #1\rvert}
\newcommand{\len}[1]{\lVert #1\rVert}
\newcommand{\R}{\mathbb{R}}
\newcommand{\x}{\mathbf{x}}
\newcommand{\y}{\mathbf{y}}
\newcommand{\z}{\mathbf{z}}

\title{Math 4310 Assignment \#1 Solutions\\University of Lethbridge, Fall 2014}
\author{Sean Fitzpatrick}
\begin{document}
 \maketitle

{\bf Due date:} Friday, September 12, by 5 pm.

\bigskip

\subsection*{Practice Problems (do not submit)}
Sorry guys; at one point I had good intentions of writing up solutions to some of these practice problems, but was defeated by the finite nature of time. If you want the solution for one of them, I suggest asking about it on Piazza.

\begin{enumerate}
 \item Let $f:A\to B$ be given and let $\{X_\alpha\}_{\alpha\in I}$ be an indexed family of subsets of $A$. Prove:
\begin{enumerate}
 \item $f(\bigcup_{\alpha\in I}X_\alpha) = \bigcup_{\alpha\in I}f(X_\alpha)$
 \item $f(\bigcap_{\alpha\in I} X_\alpha)\subseteq \bigcap_{\alpha\in I}f(X_\alpha)$ 
 \item If $f:A\to B$ is one-to-one, then $f(\bigcap_{\alpha\in I} X_\alpha)= \bigcap_{\alpha\in I}f(X_\alpha)$
\end{enumerate}
 \item Let $f:A\to B$ be given and let $\{Y_\alpha\}_{\alpha\in I}$ be an indexed family of subsets of $B$. Prove:
\begin{enumerate}
 \item $f^{-1}(\bigcup_{\alpha\in I} Y_\alpha) = \bigcup_{\alpha\in I} f^{-1}(Y_{\alpha})$
 \item $f^{-1}(\bigcap_{\alpha\in I}Y_\alpha) = \bigcap_{\alpha\in I}f^{-1}(Y_\alpha)$
 \item If $X$ is a subset of $B$, then $f^{-1}(X^c) = (f^{-1}(X))^c$, where $X^c = B\setminus X$ denotes the complement of $X$ in $B$ (and similarly $(f^{-1}(X))^c$ denotes the complement of $f^{-1}(X)$ in $A$).
 \item If $X$ is a subset of $A$ and $Y$ is a subset of $B$, then $f(X\cap f^{-1}(Y)) = f(X)\cap Y$.
\end{enumerate}
 \item Let $A$ be the set of all functions $f:[a,b]\to\R$ that are continuous on $[a,b]$. Let $B$ be the subset of $A$ consisting of all the functions possessing a continuous derivative on $[a,b]$. Let $C$ be the subset of $B$ consisting of all functions whose value at $a$ is 0. Let $d:B\to A$ be the correspondence that associates with each function in $B$ its derivative. Is the function $d$ invertible?

To each $f\in A$, let $h(f)$ be the function defined by $(h(f))(x) = \int_a^x f(t)\,dt$, for $x\in [a,b]$. Verify that $h:A\to C$. Find the function $g:C\to A$ such that these two functions are inverses of each other.
 \item Let $m,n$ be positive integers. Let $X$ be a set with $m$ distinct elements and $Y$ a set with $n$ distinct elements. How many distinct functions are there from $X$ to $Y$? Let $A$ be a subset of $X$ with $r$ distinct elements, with $0\leq r\leq m$, and $f:A\to Y$. In how many distinct ways can we extend $f$ to a function defined on all of $X$?
 \item Prove that $(\R^n,d'')$ is a metric space, where the function $d''$ is defined by the correspondence
\[
d''(\x,\y) = \sum_{i=1}^n\abs{x_i-y_i},
\]
for $\x=(x_1,\ldots, x_n),\y=(y_1,\ldots y_n)\in\R^n$. In $(\R^2,d'')$ determine the shape and position of the set of points $\x$ such that $d(\x,\y)\leq 1$ for a given point $\y\in\R^2$.
 \item Let $(X_i,d_i)$, $(Y_i,d_i')$, $i=1,\ldots, n$ be metric spaces. Let $f_i:X_i\to Y_i$ be continuous functions with respect to the given metrics. Let $X=\prod_{i=1}^nX_i$ and $Y=\prod_{i=1}^nY_i$, and make $X$ and $Y$ into metric spaces as in the previous problem. Prove that the function $F:X\to Y$ defined by
\[
 F(x_1,\ldots, x_n) = (f_1(x_1),\ldots, f_n(x_n))
\]
is continuous.
 \item Let $\delta(x,y)$ be a real-valued function on $X\times X$ for some set $X$, and suppose that for all $x,y,z\in X$ we have
\begin{enumerate}[(i)]
 \item $\delta(x,y)\geq 0$ and $\delta(x,y)=0$ if and only if $x=y$.
 \item $\delta(x,y)\leq \delta(x,z)+\delta(y,z)$.
\end{enumerate}
Deduce that, in addition, $\delta(x,y)=\delta(y,x)$, and thus that $\delta(x,y)\leq \delta(x,z)+\delta(z,y)$.
 \item If $d$ is a metric on a set $X$, show that $d_1(x,y) = \dfrac{d(x,y)}{1+d{x,y}}$ and $d_2(x,y)=\min\{d(x,y),1\}$ are also metrics on $X$.
\end{enumerate}
\subsection*{Assigned problems (to be submitted)}
\begin{enumerate}
 \item Let $A$ be a set and let $E\subseteq A$. The function $\chi_E:A\to\{0,1\}$ defined by
\[
 \chi_E(x) = \begin{cases}1, & \text{ if } x\in E\\ 0, & \text{ if } x\notin E\end{cases}
\]
is called the {\em characteristic function} of $E$. Let $E$ and $F$ be subsets of $A$.
\begin{enumerate}
 \item Show that $\chi_{E\cap F} = \chi_E\cdot \chi_F$, where $\chi_E\cdot\chi_F(x) = \chi_E(x)\chi_F(x)$.

\bigskip

We consider four cases: $x\in E$ and $x\in F$ (so $x\in E\cap F$) gives 
\[ 
\chi_{E\cap F}(x) = 1 = 1\cdot 1 = \chi_E(x)\cdot \chi_F(x). 
\]
The other three cases are $x\in E$ and $x\notin F$; $x\notin E$ and $x\in F$; and $x\notin E$ and $x\notin F$. In each case $x\notin E\cap F$, so $\chi_{E\cap F}=0$, while the right hand side is equal to $1\cdot 0 = 0\cdot 1 = 0\cdot 0 = 0$, respectively.

 \item Show that $\chi_{E\cup F} = \chi_E+\chi_F-\chi_{E\cap F}$.

\bigskip

One can argue by cases as in part (a), or proceed as follows: if we denote the complement of $B\subseteq A$ by $B^c=A\setminus B$, then we have $\chi_{B^c} = 1-\chi_B$. (If this is not clear, consider the two cases $x\in B$ and $x\notin B$ (which is equivalent to $x\in B^c$).) Using de Morgan's laws, $E\cup F = (E^c\cap F^c)^c$, so using the result from part (a), we have
\[
 \chi_{E\cup F} = 1-\chi_{E^c\cap F^c} = 1 - (1-\chi_E)(1-\chi_F) = 1 - (1 -\chi_E-\chi_F+\chi_E\cdot\chi_F) = \chi_E+\chi_F-\chi_{E\cap F}.
\]

 \item Find a similar expression for $\chi_{E\cup F\cup G}$

\bigskip

Noting that the formula from part (b) resembles the principle of inclusion-exclusion for set cardinality, we might guess that the version for the union of three sets works as well. Again, we can either make this guess, which says that
\[
 \chi_{E\cup F\cup G} = \chi_E+\chi_F+\chi_G-\chi_{E\cap F}-\chi_{E\cap G}-\chi_{F\cap G}+\chi_{E\cap F\cap G},
\]
and check all eight cases, or we can proceed as in part (b) by making use of the properties of set operations. Since the union of sets is associative, we have
\begin{align*}
 \chi_{E\cup F\cup G} & = \chi_{E\cup (F\cup G)}\\
& = \chi_E+\chi_{F\cup G}-\chi_{E\cap (F\cup G)} \text{ (using part (b))}\\
& = \chi_E+\chi_F+\chi_G-\chi_{F\cap G}-\chi_{(E\cap F)\cup (E\cap G)} \text{ ((b) again, and the distributive law)}\\
& = \chi_E+\chi_F+\chi_G-\chi_{F\cap G}-\chi_{E\cap F}-\chi_{E\cap G}+\chi_{E\cap F\cap G},
\end{align*}
where in the last line we used (b) again, and the fact that $E\cap F\cap E\cap G = E\cap F\cap G$.

\end{enumerate}
\item Let $X$ denote the set of all continuous functions $f:[a,b]\to\R$, and let $X'$ denote the set of all bounded functions $f:[a,b]\to\R$. (Note: by the Extreme Value Theorem, $X\subseteq X'$.)

\begin{enumerate}
 \item For $f,g\in X$, define $d(f,g) = \int_a^b\abs{f(t)-g(t)}\,dt$. Using appropriate theorems from calculus, prove that $(X,d)$ is a metric space.

\bigskip

It's clear that $d(f,g)\geq 0$ for all $f,g\in X$, since $\abs{f(t)-g(t)}\geq 0$ and the Riemann integral is monotonic (if $h(x)\geq k(x)$ for all $x\in [a,b]$ then $\int_a^b h\geq \int_a^b k$.) Now suppose that $d(f,g)=0$ for some $f,g\in X$. This means that
\[
 \int_a^b\abs{f(t)-g(t)}\,dt = 0.
\]
Since $f$ and $g$ are continuous, so is $\abs{f-g}$, and for {\em continuous} functions, it's true that the integral is zero if and only if the function is zero, so it follows that $\abs{f-g}=0$ and thus $f=g$. (To support this assertion, you should either cite Lemma 5.15 from the text, or give a short proof.)

Symmetry of the metric is clear, since $\abs{f(t)-g(t)}=\abs{g(t)-f(t)}$ for any $t\in [a,b]$.


The triangle inequality follows from the triangle inequality for the absolutve value and the monotonicity of the integral: since 
\[
 \abs{f(t)-g(t)}\leq \abs{f(t)-h(t)}+\abs{h(t)-g(t)} \text{ for all } t\in [a,b] \text{ and any } f,g,h\in X,
\]
it follows that $d(f,g)\leq d(f,h)+d(h,g)$.

 \item For $f,g\in X'$, define $d'(f,g) = \sup_{x\in [a,b]}(\abs{f(x)-g(x)})$. Prove that $(X',d')$ is a metric space.

\bigskip

We see that $d'(f,g)\geq 0$ for any $f,g\in X'$, since $\sup\abs{f(t)-g(t)}\geq \abs{f(x)-g(x)}\geq 0$ for any $x$, by defintion of supremum (it's an upper bound). Moreover, if $d'(f,g)=0$, then we have
\[
 0=\sup\{\abs{f(t)-g(t)}\} \geq \abs{f(x)-g(x)}\geq 0 \text{ for all } x\in [a,b],
\]
so it follows that $f(x)=g(x)$ for all $x\in [a,b]$, and thus $f=g$.

Finally, for the triangle inequality, we note that for any $x\in [a,b]$, and $f,g,h\in X'$, we have
\[
 \abs{f(x)-g(x)} \leq \abs{f(x)-h(x)}+\abs{h(x)-g(x)}\leq d(f,h)+d(h,g),
\]
using the fact that the supremum is an upper bound. Since the right-hand side is an upper bound for $\abs{f(x)-g(x)}$ and $d(f,g)$ is the {\em least} upper bound, we have $d(f,g)\leq d(f,h)+d(h,g)$, as required.


\noindent{\bf Remark:} Note the argument given above for the triangle inequality. This is a fairly standard technique when suprema are involved; note how it makes use of both parts of the definition of supremum. In general, inequalities and least upper bounds are a combination that needs to be handled with care. 

One other comment: note that the functions $f,g,\ldots$ are the {\em points} in our metric space in this case. Thus, you don't want to write a conclusion such as `$f(x)=g(x)'$, which only asserts the equality of two numbers (the values of the functions at $x$). You can note that this holds for all $x\in [a,b]$, but from this you must conclude that therefore, $f=g$.

 \item Since $X\subseteq X'$, the metric $d'$ defines a metric on $X$ by restriction. Compare the two metrics $d$ and $d'$ on $X$.

\bigskip

Since any continuous function attains its maximum on $[a,b]$, by the Extreme Value Theorem, we have that
\[
 d'(f,g) = \max_{x\in [a,b]}\abs{f(x)-g(x)}.
\]
(We can replace supremum by maximum in the definition of $d'(f,g)$. Recall also that if $f(x)\leq M$ for all $x\in [a,b]$, then
\[
 \int_a^b f(x)\,dx \leq M(b-a).
\]
Thus, we obtain the comparison $d(f,g)\leq (b-a)\cdot d'(f,g)$, for any functions $f,g\in X$. Note however that there is no corresponding comparison in the other direction. Consider the family of functions $f_n$, $n\geq 2$, on $[0,1]$ such that
\[
 f_n(x) = 0 \text{ for } 0\leq x\leq \frac{1}{n+1} \text{ and } \frac{1}{n-1}\leq x\leq 1,
\]
and such that $f_n(1/n)=n$. We can make each $f_n$ continuous by taking the remainder of the graph to consist of the straight line from the point $(1/(n+1),0)$ to $(1/n,n)$ and then from $(1/n,n)$ back to $(1/(n-1),0)$. (The graphs are thus a sequence of continually taller and narrower triangles.)

For each $n$ we have $\displaystyle \int_0^1 f_n(x)\,dx = \frac{n}{n^2-1}<1$, so $d(f_n,0)<1$ for all $n$ (and in fact $d(f_n,0)\to 0$ as $n\to \infty$), while $d'(f_n,0) = n$ for each $n$. Thus no matter what constant $K$ we choose, the inequality $d'(f,g)\leq K d(f,g)$ can be violated: take $f=f_n$, $g=0$, and $n>K$.
\end{enumerate}
 \item Given metric spaces $X_1,\ldots, X_n$ with metrics $d_1,\ldots, d_n$ respectively, let $X = \prod_{i=1}^nX_i$. Then the function $d:X\times X\to \R$ defined by 
\[
 d(\x,\y) = \max_{1\leq i\leq n}\{d_i(x_i,y_i)\}.
\]
makes $(X,d)$ into a metric space. Let $d$ continue to denote the metric on $\R^n$ defined by this result, where each $d_i$ is the usual absolute value metric on $\R$, let $d'$ be the standard Euclidean metric on $\R^n$, and let $d''$ be the metric defined in problem \#5 from the practice problems above. Prove that for each pair of points $\x,\y\in\R^n$, we have
\begin{align*}
 d(\x,\y)&\leq d'(\x,\y)\leq \sqrt{n}\cdot d(\x,\y),\\
 d(\x,\y)&\leq d''(\x,\y)\leq n\cdot d(\x,\y).
\end{align*}

\bigskip

Let $d(\x,\y) = \max\{\abs{x_i-y_i}\}$ and let $d'(\x,\y) = \sqrt{\sum(x_i-y_i)^2}$, where $i=1,2,\ldots, n$ for both. Suppose $\max\{\abs{x_i-y_i}\} = \abs{x_k-y_k}$, for some $k$ with $1\leq k\leq n$. Then
\[
d(\x,\y) = \abs{x_k-y_k} = \sqrt{(x_k-y_k)^2}\leq \sqrt{\sum_{i=1}^n(x_i-y_i)^2} \leq \sqrt{n(x_k-y_k)^2}=\sqrt{n}\cdot d(\x,\y)
\]
Similarly, 
\[
 \abs{x_k-y_k}\leq \sum_{i=1}^n\abs{x_i-y_i}\leq \sum_{i=1}^n\abs{x_k-y_k} = n\abs{x_k-y_k},
\]
which shows that $d(\x,\y)\leq d''(\x,\y)\leq n\cdot d(\x,\y)$.

 \item Define a function $d:\R^2\times\R^2\to \R$ by
\[
 d(x,y) = \begin{cases}
           \len{\x-\y}, & \text{ if } \x=c\y \text{ for some } c\in\R\\
           \len{\x}+\len{\y} & \text{ otherwise}
          \end{cases}
\]
Verify that $d$ defines a metric on $\R^2$, and describe the open balls $B^d_a(\x) = \{\y\in\R^2 \,|\, d(\x,\y)<a\}$ with respect to this metric. (Hint: if you get stuck, this metric is known as the ``Paris Metro'' metric. This should let you find help online -- but cite your sources! Once you figure it out, take a look at a map of the Paris subway system to understand where the name comes from.)

\bigskip

Before checking that $d$ is a metric, note that the two cases are as follows: if $\x$ and $\y$ lie on the same line through the origin, then $d(\x,\y)$ is just the usual Euclidean distance. Otherwise, $d(\x,\y)$ is the distance from $\x$ to the origin, {\em plus} the distance from the origin to $\y$: if the points are not on the same line you have to get from one to the other via the origin. The name ``Paris Metro'' metric comes from the early days of the Paris subway system, which consisted of several spur lines originating at a central station. If you wanted to get from a station on one line to a station on another, you had to pass through the centre. (They've since added more lines that don't pass through the central station.)

It's clear that both $\len{\x-\y}$ and $\len{\x}+\len{\y}$ are nonnegative, so $d(\x,\y)=0$, and if either of these terms vanish, then we must have $\x=\y$. (In the latter case, we in fact must have $\x=\y=(0,0)$.) Thus $d(\x,\y)=0$ implies $\x=\y$. The fact that $d(\x,\y)=d(\y,\x)$ is also immediate from the definition. For the triangle inequality, we must check a number of cases.

There are three possibilities: either all three points are colinear, or two points are, or none are. We note that for any $\x, \y, \z$, we have
\[
 \len{\x-\y}\leq \len{\x-\z}+\len{\z-\y}\leq \len{\x}+\len{\z}+\len{\z}+\len{\y},
\]
so the triangle inequality holds for $\x$ and $\y$ colinear, whether $\z$ lies on the same line or not. If $\x$ and $\y$ are not colinear, then either $\z$ lies on the same line as one, or all three lie on different lines from the origin. For the first case, assume $\z$ lies on the same line as $\x$, without loss of generality (the proof being the same if it's colinear with $\y$). Then we have
\begin{align*}
 \len{\x}+\len{y}  &= \len{\x}-\len{\z}+\len{\z}+\len{y}\\
&\leq \len{\x-\z}+\len{\z}+\len{\y}\\
&\leq \len{\x}+\len{\z}+\len{\z}+\len{y}.
\end{align*}
The second line above establishes the triangle inequality in the first case, and the third line gives the result for the second case. Thus, the triangle inequality holds for all possibilities.

We now consider the open neighbourhoods of a point $\x\in\R^2$. If $\x$ is the origin, then $d(\x,\y) = \len{\y}$ and an $\epsilon$-neighbourhood is just an open disc of radius $\epsilon$ centred at the origin. If $\x\neq (0,0)$, there are two possibilities. If $\epsilon<\len{x}$, then $N_\epsilon(\x)$ consists of the points on the ray from the origin through $\x$ whose distance from $\x$ is less then $\epsilon$. If $\len{x}<\epsilon$, then $N_\epsilon(\x)$ consists of the ray from the origin through $\x$, up to a distance of $\len{\x}+\epsilon$ (i.e. from $\x$ to the origin, and away from the origin a distance $\epsilon$ from $\x$), plus an open disc of radius $\epsilon-\len{\x}$ around the origin. (We're allowed to travel a distance $\epsilon$ from $\x$; once we reach the origin we've gone a distance of $\len{\x}$ so we still have room to travel a distance $\epsilon-\len{\x}$ in any direction.

Summing up, the neighbourhoods can be discs ($\x = (0,0)$), line segments ($0<\epsilon<\len{\x}$), or ``lollipops'' ($0<\len{\x}<\epsilon$).
\end{enumerate}
\end{document}
 
